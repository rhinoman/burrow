# Burrow configuration — https://github.com/jcadam/burrow
#
# This is the config generated by `gd quickstart`. It uses the free
# NWS weather API (no API key required) as a working demo.
#
# To use this manually: copy to ~/.burrow/config.yaml

services:
  - name: weather-gov
    type: rest
    endpoint: https://api.weather.gov

    # weather.gov requires a User-Agent identifying your application.
    # No API key needed — it's a free public service.
    auth:
      method: user_agent
      value: "burrow/1.0 (quickstart-demo@example.com)"

    tools:
      - name: forecast
        description: "7-day forecast for Denver/Boulder, CO"
        method: GET
        path: /gridpoints/BOU/62,60/forecast
        # To change location:
        #   1. Find your grid point: https://api.weather.gov/points/{lat},{lon}
        #   2. Use the gridId/gridX/gridY from the response
        #   3. Replace path: /gridpoints/{gridId}/{gridX},{gridY}/forecast

      - name: alerts
        description: "Active weather alerts for Colorado"
        method: GET
        path: /alerts/active?area=CO
        # Change "CO" to your state abbreviation

  # --- Add real services below ---
  #
  # - name: sam-gov
  #   type: rest
  #   endpoint: https://api.sam.gov
  #   auth:
  #     method: api_key
  #     key: ${SAM_API_KEY}
  #   tools:
  #     - name: search_opportunities
  #       description: "Search active contract opportunities"
  #       method: GET
  #       path: /opportunities/v2/search
  #       params:
  #         - name: naics
  #           type: string
  #           maps_to: api.ncode
  #
  # - name: edgar
  #   type: rest
  #   endpoint: https://efts.sec.gov
  #   auth:
  #     method: user_agent
  #     value: "burrow/1.0 you@example.com"
  #   tools:
  #     - name: company_filings
  #       description: "Search SEC filings"
  #       method: GET
  #       path: /LATEST/search-index
  #       params:
  #         - name: keywords
  #           type: string
  #           maps_to: q

# --- LLM providers ---
#
# Quickstart auto-detects Ollama and generates a section like this:
llm:
  providers:
    - name: local/llama3:latest
      type: ollama
      endpoint: http://localhost:11434
      model: "llama3:latest"
      privacy: local

# Additional providers (uncomment to add):
#
#     - name: openrouter/openai/gpt-4o-mini
#       type: openrouter
#       api_key: ${OPENROUTER_API_KEY}
#       model: openai/gpt-4o-mini
#       privacy: remote

privacy:
  strip_referrers: true
  minimize_requests: true
  # randomize_user_agent: true    # rotate user agents (breaks user_agent auth)
  # default_proxy: socks5://...   # route all traffic through proxy
  # strip_attribution_for_remote: true  # strip service names from remote LLM prompts
